{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Compare_kmean_gmm.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMX3zOufnA457g090t2C8UR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"BAs7oNqLKx7t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649000641924,"user_tz":240,"elapsed":131977,"user":{"displayName":"Theo Zeng","userId":"13888946804168493632"}},"outputId":"7fc9944e-243c-4282-db4b-7d5f0951404d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","num_pts =  10000   dim =  100\n","WARNING:tensorflow:From <ipython-input-2-6da27edfee55>:65: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n","Instructions for updating:\n","keep_dims is deprecated, use keepdims instead\n","WARNING:tensorflow:From <ipython-input-2-6da27edfee55>:73: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n","Instructions for updating:\n","keep_dims is deprecated, use keepdims instead\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","K= 5    |k_mean_valid_loss = 21.54334444037717    |GMM_valid_loss= 10.834202951545155\n","num_pts =  10000   dim =  100\n","K= 10    |k_mean_valid_loss = 21.24050911505024    |GMM_valid_loss= 12.697508813381338\n","num_pts =  10000   dim =  100\n","K= 15    |k_mean_valid_loss = 21.042035154237134    |GMM_valid_loss= 9.654315040879087\n","num_pts =  10000   dim =  100\n","K= 20    |k_mean_valid_loss = 20.992872165406673    |GMM_valid_loss= 8.715772553817882\n","num_pts =  10000   dim =  100\n","K= 30    |k_mean_valid_loss = 20.71745446427862    |GMM_valid_loss= 6.316014218609361\n"]}],"source":["%tensorflow_version 1.x\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/ECE421_Lab3/')\n","import helper as hlp\n","\n","\n","# Loading data\n","def load_dataset(is_2D,is_valid = False):\n","  data = None\n","  if(is_2D):\n","    data = np.load('/content/drive/MyDrive/Colab Notebooks/ECE421_Lab3/data2D.npy')\n","  else:\n","    data = np.load('/content/drive/MyDrive/Colab Notebooks/ECE421_Lab3/data100D.npy')\n","  [num_pts, dim] = np.shape(data)\n","  print(\"num_pts = \",num_pts,\"  dim = \",dim)\n","\n","  # For Validation set\n","  if is_valid:\n","    valid_batch = int(num_pts / 3.0)\n","    np.random.seed(45689)\n","    rnd_idx = np.arange(num_pts)\n","    np.random.shuffle(rnd_idx)\n","    val_data = data[rnd_idx[:valid_batch]]\n","    data = data[rnd_idx[valid_batch:]]\n","    return data, val_data\n","  else:\n","    return data\n","\n","\n","# Distance function for K-means\n","def distance_func(X, mu):\n","  \"\"\" Inputs:\n","        X: is an NxD matrix (N observations and D dimensions)\n","        mu: is an KxD matrix (K means and D dimensions)\n","        \n","      Output:\n","        pair_dist: is the squared pairwise distance matrix (NxK)\n","  \"\"\"\n","  X = tf.expand_dims(X,1) #shape becomes (N,1,D)\n","  square = tf.square((X-mu))\n","  pair_dist = tf.reduce_sum(square,2) #change shape to (N,K)\n","  \n","  return pair_dist\n","\n","#============================== Helper ===========================================\n","def reduce_logsumexp(input_tensor, reduction_indices=1, keep_dims=False):\n","  \"\"\"Computes the sum of elements across dimensions of a tensor in log domain.\n","\n","     It uses a similar API to tf.reduce_sum.\n","\n","  Args:\n","    input_tensor: The tensor to reduce. Should have numeric type.\n","    reduction_indices: The dimensions to reduce. \n","    keep_dims: If true, retains reduced dimensions with length 1.\n","  Returns:\n","    The reduced tensor.\n","  \"\"\"\n","  max_input_tensor1 = tf.reduce_max(\n","      input_tensor, reduction_indices, keep_dims=keep_dims)\n","  max_input_tensor2 = max_input_tensor1\n","  if not keep_dims:\n","    max_input_tensor2 = tf.expand_dims(max_input_tensor2, reduction_indices)\n","  return tf.log(\n","      tf.reduce_sum(\n","          tf.exp(input_tensor - max_input_tensor2),\n","          reduction_indices,\n","          keep_dims=keep_dims)) + max_input_tensor1\n","\n","\n","def logsoftmax(input_tensor):\n","  \"\"\"Computes normal softmax nonlinearity in log domain.\n","\n","     It can be used to normalize log probability.\n","     The softmax is always computed along the second dimension of the input Tensor.     \n","\n","  Args:\n","    input_tensor: Unnormalized log probability.\n","  Returns:\n","    normalized log probability.\n","  \"\"\"\n","  return input_tensor - reduce_logsumexp(input_tensor, reduction_indices=0, keep_dims=True)\n","\n","#============================================= GMM ================================================\n","\n","def log_gauss_pdf(X, mu, sigma):\n","  \"\"\" Inputs: \n","      X: N X D\n","      mu: K X D\n","      sigma: K X 1\n","\n","    Outputs:\n","      log Gaussian PDF (N X K)\n","  \"\"\"\n","  part1 = -(0.5) * tf.log(2 * np.pi * tf.transpose(sigma)**2)\n","  pair_dist = distance_func(X, mu)\n","  part2 = -(0.5) * tf.square(pair_dist) / (tf.transpose(sigma)**2)\n","  return part1 + part2\n","\n","def log_posterior(log_PDF, log_pi):\n","  \"\"\" Inputs:\n","      log_PDF: log Gaussian PDF N X K\n","      log_pi: K X 1\n","\n","    Outputs\n","      log_post: N X K\n","  \"\"\"\n","  log_numerator = log_PDF + tf.squeeze(log_pi)\n","  log_denominator = reduce_logsumexp(log_numerator, keep_dims=True)\n","  answer = log_numerator - log_denominator\n","  return answer\n","\n","\n","\n","def build_GMM_Graph(K,D):\n","  #define variables\n","  X = tf.placeholder(tf.float32, shape=(None,D))\n","  mu = tf.Variable(tf.random_normal([K, D], stddev = 1)) \n","  # theta is unconstrained parameter, sigma = exp(phi) with [0 - inf] \n","  theta = tf.Variable(tf.random_normal([K, 1], stddev = 1))\n","  sigma = tf.exp(theta)\n","\n","  # phi is unconstrained parameter, acheive constraint for pi\n","  phi = tf.Variable(tf.random_normal([K, 1], stddev = 1))\n","  log_pi = logsoftmax(phi)\n","\n","  log_pdf = log_gauss_pdf(X,mu,sigma)\n","\n","  #defien loss & optimizer\n","  loss= - tf.reduce_sum(reduce_logsumexp(log_pdf + tf.squeeze(log_pi), keep_dims=True))\n","  optimizer = tf.train.AdamOptimizer(learning_rate=0.1, beta1=0.9, beta2=0.99, epsilon=1e-5).minimize(loss)\n","  return X,mu,sigma,optimizer,loss,log_pdf,log_pi\n","\n","def Train_GMM(dataset,K,epochs,valid_data):\n","  D = dataset.shape[1]\n","  X,mu,sigma,optimizer,loss,log_pdf,log_pi = build_GMM_Graph(K,D)\n","  loss_List = []\n","  best_mu = None\n","  best_sigma = None\n","  best_pi = None\n","  cluster = None\n","  valid_loss = None\n","  with tf.Session() as sess:\n","      sess.run(tf.global_variables_initializer())\n","      for i in range(epochs):\n","          best_mu,best_sigma,best_pi,opt = sess.run([mu,sigma,log_pi,optimizer], feed_dict={X: dataset})\n","          Loss = sess.run(loss, feed_dict={X: dataset})\n","          loss_List.append(Loss/dataset.shape[0])\n","\n","      # end of traning find cluster\n","      # Returns the index with the smallest value across axes of a tensor\n","      cluster = sess.run(tf.argmax(log_posterior(log_pdf,log_pi),1),feed_dict={X:dataset})\n","      # end of training find validation loss\n","      valid_loss = sess.run(loss, feed_dict={X: valid_data})\n","      valid_loss = valid_loss/valid_data.shape[0]\n","  return loss_List,best_mu,best_sigma,best_pi,cluster,valid_loss\n","\n","#============================================= GMM ================================================\n","\n","def build_k_means_Graph(K,D):\n","  X = tf.placeholder(tf.float64, shape=(None,D))\n","  mu = tf.Variable(tf.truncated_normal((K, D), mean=0, stddev=1, dtype=tf.float64), trainable=True)\n","  loss = tf.reduce_sum(tf.reduce_min(distance_func(X,mu), axis=1))\n","  optimizer = tf.train.AdamOptimizer(learning_rate=0.1, beta1=0.9, beta2=0.99, epsilon=1e-5).minimize(loss)\n","  return X,mu,loss,optimizer\n","\n","\n","def Train_K_means(dataset,K,epochs,valid_data):\n","  D = dataset.shape[1]\n","  X,mu,loss,optimizer = build_k_means_Graph(K,D)\n","  loss_List = []\n","  best_mu = None\n","  cluster = None\n","  valid_loss = None\n","  with tf.Session() as sess:\n","      sess.run(tf.global_variables_initializer())\n","      for i in range(epochs):\n","          best_mu,opt = sess.run([mu,optimizer], feed_dict={X: dataset})\n","          Loss = sess.run(loss, feed_dict={X: dataset})\n","          loss_List.append(Loss/dataset.shape[0])\n","\n","      # end of traning find cluster\n","      # Returns the index with the smallest value across axes of a tensor\n","      cluster = sess.run(tf.argmin(distance_func(X, best_mu), 1),feed_dict={X:dataset})\n","      # end of training find validation loss\n","      valid_loss = sess.run(loss, feed_dict={X: valid_data})\n","      valid_loss = valid_loss/valid_data.shape[0]\n","  return loss_List,best_mu,cluster,valid_loss\n","\n","\n","#============================================ Test ===============================================\n","def Q3():\n","  K_list = [5,10,15,20,30]\n","  k_mean_valid_loss = None\n","  GMM_valid_loss = None\n","  for K in K_list:\n","    data, val_data = load_dataset(False,True)\n","    loss_List,best_mu,cluster,           k_mean_valid_loss = Train_K_means(data,K,100,val_data)\n","    loss_List,best_mu,best_sigma,best_pi,cluster,  GMM_valid_loss = Train_GMM(data,K,100,val_data)\n","    print(\"K=\",K,\"   |k_mean_valid_loss =\",k_mean_valid_loss,\"   |GMM_valid_loss=\",GMM_valid_loss)\n","  return  \n","\n","Q3()\n","\n","\n"]},{"cell_type":"markdown","source":["# 新段落"],"metadata":{"id":"PiSDqzYEuEr3"}},{"cell_type":"markdown","source":["# 新段落"],"metadata":{"id":"3MYsy3iKtxRz"}},{"cell_type":"markdown","source":["# 新段落"],"metadata":{"id":"GsomOOobtrE8"}}]}